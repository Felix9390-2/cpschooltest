<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <title>Voice Chat — VU + SpeechRecognition (Original UI Restored)</title>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        background: #000;
        color: #fff;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
        height: 100vh;
        display: flex;
        flex-direction: column;
      }
      .status {
        position: fixed;
        top: 10px;
        left: 50%;
        transform: translateX(-50%);
        font-size: 12px;
        color: #aaa;
        background: rgba(10, 10, 10, 0.95);
        padding: 6px 12px;
        border-radius: 12px;
        border: 1px solid #111;
        z-index: 999;
      }
      .chat-container {
        flex: 1;
        overflow-y: auto;
        padding: 20px;
        display: flex;
        flex-direction: column;
        gap: 10px;
      }
      .chat-container::-webkit-scrollbar {
        width: 6px;
      }
      .chat-container::-webkit-scrollbar-thumb {
        background: #222;
        border-radius: 6px;
      }
      .message {
        background: #0a0a0a;
        border: 1px solid #1a1a1a;
        border-radius: 12px;
        padding: 10px 14px;
        font-size: 14px;
        line-height: 1.4;
        color: #ccc;
        max-width: 78%;
        word-wrap: break-word;
        display: inline-block;
      }
      .message.user {
        align-self: flex-end;
        background: linear-gradient(180deg, #062, #093);
        color: #fff;
      }
      .message.assistant {
        align-self: flex-start;
        background: #0a0a0a;
        color: #ddd;
      }
      .message.interim {
        opacity: 0.8;
        border-style: dashed;
        color: #ffdb6b;
      }
      .meta {
        font-size: 11px;
        color: #888;
        margin-top: 6px;
      }
      .input-container {
        display: flex;
        gap: 10px;
        padding: 12px 16px;
        background: #000;
        border-top: 1px solid #111;
      }
      input[type="text"] {
        flex: 1;
        background: #0a0a0a;
        border: 1px solid #1a1a1a;
        border-radius: 20px;
        padding: 10px 16px;
        color: #fff;
        font-size: 14px;
        font-family: inherit;
      }
      .btn {
        width: 44px;
        height: 44px;
        border-radius: 50%;
        border: 1px solid #1a1a1a;
        background: #0a0a0a;
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.12s;
        flex-shrink: 0;
      }
      .btn:hover {
        transform: scale(1.03);
      }
      .btn.recording {
        background: #300;
        border-color: #600;
        box-shadow: 0 0 12px rgba(255, 0, 0, 0.12);
      }
      .icon {
        width: 18px;
        height: 18px;
        fill: #999;
      }
      .btn:hover .icon {
        fill: #ccc;
      }
      .vu-wrap {
        padding: 12px;
        background: #050505;
        border-top: 1px solid #111;
        display: flex;
        align-items: center;
        gap: 12px;
      }
      #vu {
        width: 160px;
        height: 12px;
        background: #111;
        border-radius: 6px;
        overflow: hidden;
      }
      #level {
        height: 100%;
        width: 0%;
        background: linear-gradient(90deg, #4caf50, #ff9800, #f44336);
        transition: width 0.06s linear;
      }
      #devLabel {
        color: #aaa;
        font-size: 13px;
      }
      @media (max-width: 768px) {
        .message {
          max-width: 92%;
        }
        #vu {
          width: 120px;
        }
      }
    </style>
  </head>
  <body>
    <div class="status" id="status">idle</div>

    <div class="chat-container" id="chat"></div>

    <div class="vu-wrap">
      <div id="vu"><div id="level"></div></div>
      <div class="muted" id="devLabel">device: —</div>
    </div>

    <div class="input-container">
      <button class="btn" id="micBtn" title="Toggle mic">
        <svg class="icon" viewBox="0 0 24 24">
          <path
            d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z"
          />
          <path
            d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"
          />
        </svg>
      </button>

      <input type="text" id="textInput" placeholder="Type a message..." />
      <button class="btn" id="sendBtn" title="Send">
        <svg class="icon" viewBox="0 0 24 24">
          <path d="M2.01 21L23 12 2.01 3 2 10l15 2-15 2z" />
        </svg>
      </button>
    </div>

    <script>
      /*
  Voice Chat — Original UI restored + VU meter + SpeechRecognition stable-ish logic.
  How to use:
    • Deploy to Render root (./index.html)
    • Open production URL in a NEW TAB (not preview iframe)
    • Click mic button (allow microphone in the browser prompt)
    • Speak — interim text will appear; final results become messages.
*/

      (() => {
        // ---------- DOM ----------
        const chat = document.getElementById("chat");
        const textInput = document.getElementById("textInput");
        const sendBtn = document.getElementById("sendBtn");
        const micBtn = document.getElementById("micBtn");
        const statusEl = document.getElementById("status");
        const devLabel = document.getElementById("devLabel");
        const levelEl = document.getElementById("level");

        // ---------- state ----------
        const STORAGE_KEY = "voiceChatMessages_v1";
        let recognition = null;
        let interimElement = null;
        let shouldKeepRecording = false; // user intent: auto-restart
        let isRecording = false; // recognition.onstart indicator
        let restarting = false; // avoid concurrent safe restarts

        // audio / VU state
        let micStream = null;
        let audioCtx = null;
        let analyser = null;
        let sourceNode = null;
        let rafId = null;

        // ---------- history ----------
        function loadHistory() {
          try {
            const raw = localStorage.getItem(STORAGE_KEY);
            if (!raw) return [];
            return JSON.parse(raw);
          } catch (e) {
            console.warn("history load failed", e);
            return [];
          }
        }
        function saveHistory(arr) {
          try {
            localStorage.setItem(STORAGE_KEY, JSON.stringify(arr));
          } catch (e) {
            console.warn(e);
          }
        }
        function pushToHistory(obj) {
          const arr = loadHistory();
          arr.push(obj);
          saveHistory(arr);
        }
        function renderHistory() {
          const arr = loadHistory();
          chat.innerHTML = "";
          arr.forEach((msg) =>
            renderMessageDom(msg.text, msg.sender, msg.ts, msg.interim)
          );
          scrollToBottom();
        }

        // ---------- render ----------
        function escapeHtml(s) {
          return String(s)
            .replace(/&/g, "&amp;")
            .replace(/</g, "&lt;")
            .replace(/>/g, "&gt;");
        }

        function renderMessageDom(
          text,
          sender = "assistant",
          timestamp = Date.now(),
          isInterim = false
        ) {
          if (isInterim) {
            if (!interimElement) {
              interimElement = document.createElement("div");
              interimElement.className = `message assistant interim`;
              chat.appendChild(interimElement);
            }
            interimElement.textContent = text;
            scrollToBottom();
            return interimElement;
          } else {
            if (interimElement) {
              interimElement.remove();
              interimElement = null;
            }
            const msg = document.createElement("div");
            msg.className = `message ${
              sender === "user" ? "user" : "assistant"
            }`;
            msg.innerHTML = `<div class="body">${escapeHtml(
              text
            )}</div><div class="meta">${new Date(
              timestamp
            ).toLocaleTimeString()}</div>`;
            chat.appendChild(msg);
            scrollToBottom();
            return msg;
          }
        }

        function addMessage(text, sender = "assistant", isInterim = false) {
          if (isInterim) {
            renderMessageDom(text, sender, Date.now(), true);
            return;
          }
          renderMessageDom(text, sender, Date.now(), false);
          pushToHistory({ text, sender, ts: Date.now(), interim: false });

          // Speak typed messages (user messages) out loud
          if (sender === "user" && "speechSynthesis" in window) {
            // Cancel any ongoing speech first
            window.speechSynthesis.cancel();
            const u = new SpeechSynthesisUtterance(text);
            u.rate = 1;
            u.pitch = 1;
            u.volume = 1;
            window.speechSynthesis.speak(u);
          }
        }

        function scrollToBottom() {
          requestAnimationFrame(() => {
            chat.scrollTop = chat.scrollHeight;
          });
        }

        // ---------- Status & debug ----------
        function setStatus(txt) {
          statusEl.innerHTML = txt;
          console.debug("[STATUS]", txt);
        }
        function dbg(...args) {
          console.debug("[VOICE_DBG]", ...args);
        }

        // ---------- VU + getUserMedia ----------
        async function startMicAndVu() {
          setStatus("requesting mic...");
          try {
            micStream = await navigator.mediaDevices.getUserMedia({
              audio: { echoCancellation: true, noiseSuppression: true },
            });
            setStatus("mic active");
            const t = micStream.getAudioTracks()[0];
            devLabel.textContent = "device: " + (t.label || "unknown");

            // AudioContext & analyser
            audioCtx = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioCtx.createAnalyser();
            analyser.fftSize = 256;
            sourceNode = audioCtx.createMediaStreamSource(micStream);
            sourceNode.connect(analyser);

            const data = new Uint8Array(analyser.frequencyBinCount);
            function draw() {
              analyser.getByteFrequencyData(data);
              let sum = 0;
              for (let i = 0; i < data.length; i++) sum += data[i] * data[i];
              const rms = Math.sqrt(sum / data.length);
              const pct = Math.min(100, Math.round((rms / 128) * 100));
              levelEl.style.width = pct + "%";
              rafId = requestAnimationFrame(draw);
            }
            draw();
            return true;
          } catch (err) {
            setStatus("mic error");
            dbg("getUserMedia failed", err);
            return false;
          }
        }

        function stopMicAndVu() {
          if (rafId) cancelAnimationFrame(rafId);
          if (sourceNode)
            try {
              sourceNode.disconnect();
            } catch (e) {}
          if (analyser)
            try {
              analyser.disconnect();
            } catch (e) {}
          if (audioCtx && audioCtx.state !== "closed")
            audioCtx.close().catch(() => {});
          if (micStream) micStream.getTracks().forEach((t) => t.stop());
          devLabel.textContent = "device: —";
          levelEl.style.width = "0%";
          setStatus("idle");
        }

        // ---------- SpeechRecognition helpers ----------
        function srSupported() {
          return (
            "SpeechRecognition" in window || "webkitSpeechRecognition" in window
          );
        }

        function createRecognition() {
          if (!srSupported()) return null;
          const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
          const r = new SR();
          r.continuous = true;
          r.interimResults = true;
          r.lang = "en-US";

          r.onstart = () => {
            isRecording = true;
            micBtn.classList.add("recording");
            setStatus(
              '<span style="display:inline-block;width:8px;height:8px;background:#f55;border-radius:50%;margin-right:8px;vertical-align:middle;"></span>recording'
            );
            dbg("sr onstart");
          };

          r.onresult = (event) => {
            let interim = "",
              final = "";
            for (let i = event.resultIndex; i < event.results.length; i++) {
              const t = event.results[i][0].transcript;
              if (event.results[i].isFinal) final += t + " ";
              else interim += t + " ";
            }
            if (interim) {
              addMessage(interim.trim(), "assistant", true);
            } else {
              if (interimElement) {
                interimElement.remove();
                interimElement = null;
              }
            }
            if (final) {
              addMessage(final.trim(), "assistant", false);
            }
          };

          // additional debug events
          r.onaudiostart = () => dbg("onaudiostart");
          r.onaudioend = () => dbg("onaudioend");
          r.onsoundstart = () => dbg("onsoundstart");
          r.onsoundend = () => dbg("onsoundend");
          r.onspeechstart = () => dbg("onspeechstart");
          r.onspeechend = () => dbg("onspeechend");

          r.onerror = (e) => {
            dbg("sr error", e);
            const errName =
              e && e.error ? e.error : e && e.message ? e.message : "unknown";
            setStatus("sr error: " + errName);

            // transient errors — attempt safe restart if user requested continuous recording
            if (
              shouldKeepRecording &&
              (errName === "aborted" ||
                errName === "no-speech" ||
                errName === "audio-capture" ||
                errName === "network" ||
                errName === "not-allowed")
            ) {
              dbg("transient sr error, scheduling restart", errName);
              safeRestartRecognition(300);
              return;
            }

            // otherwise, shut down recording state
            shouldKeepRecording = false;
            isRecording = false;
            try {
              r.stop();
            } catch (e) {}
            micBtn.classList.remove("recording");
          };

          r.onend = () => {
            dbg(
              "sr onend. isRecording=",
              isRecording,
              "shouldKeepRecording=",
              shouldKeepRecording
            );
            isRecording = false;
            micBtn.classList.remove("recording");
            if (shouldKeepRecording) {
              safeRestartRecognition(200);
            } else {
              setStatus("idle");
            }
          };

          return r;
        }

        function safeRestartRecognition(delay = 250) {
          if (restarting) {
            dbg("already restarting - skip");
            return;
          }
          restarting = true;
          setTimeout(() => {
            try {
              if (recognition) {
                try {
                  recognition.onresult = null;
                  recognition.onend = null;
                  recognition.onerror = null;
                } catch (e) {
                  dbg("cleanup failed", e);
                }
                try {
                  recognition.stop();
                } catch (e) {}
              }
            } catch (e) {
              dbg("cleanup outer failed", e);
            }

            recognition = createRecognition();
            if (!recognition) {
              setStatus("SpeechRecognition unsupported");
              restarting = false;
              return;
            }

            try {
              if (shouldKeepRecording) {
                recognition.start();
                dbg("restarted recognition");
              } else {
                dbg("shouldKeepRecording false; not restarting");
              }
            } catch (e) {
              dbg("restart start threw", e);
              setTimeout(() => {
                try {
                  if (shouldKeepRecording && recognition) recognition.start();
                } catch (e2) {
                  dbg("final restart failed", e2);
                  setStatus("sr restart failed");
                }
              }, 600);
            } finally {
              restarting = false;
            }
          }, delay);
        }

        // ---------- control flows ----------
        async function startRecordingFlow() {
          // ensure mic permission via getUserMedia
          const ok = await startMicAndVu();
          if (!ok) {
            dbg("start aborted: no mic");
            return;
          }

          // enumerate devices for debug
          try {
            const devs = await navigator.mediaDevices.enumerateDevices();
            dbg("devices", devs);
          } catch (e) {
            dbg("enumerateDevices failed", e);
          }

          if (!srSupported()) {
            setStatus("SpeechRecognition not supported in this browser");
            dbg("SpeechRecognition not supported");
            return;
          }

          shouldKeepRecording = true;

          // create and start recognition
          recognition = createRecognition();
          if (!recognition) {
            dbg("createRecognition returned null");
            return;
          }

          try {
            recognition.start();
            dbg("requested recognition.start()");
          } catch (e) {
            dbg("recognition.start threw", e);
            safeRestartRecognition(300);
          }
        }

        function stopRecordingFlow() {
          shouldKeepRecording = false;
          if (recognition) {
            try {
              recognition.stop();
            } catch (e) {
              dbg("recognition stop threw", e);
            }
            recognition = null;
          }
          stopMicAndVu();
          micBtn.classList.remove("recording");
          setStatus("idle");
        }

        // ---------- devicechange handling ----------
        if (
          navigator.mediaDevices &&
          typeof navigator.mediaDevices.addEventListener === "function"
        ) {
          navigator.mediaDevices.addEventListener("devicechange", async () => {
            dbg("devicechange fired");
            try {
              const devs = await navigator.mediaDevices.enumerateDevices();
              dbg("devicechange devices", devs);
            } catch (e) {
              dbg("enumerateDevices error", e);
            }

            if (shouldKeepRecording) {
              dbg("device changed while recording; restarting flows");
              try {
                stopRecordingFlow();
              } catch (e) {
                dbg("stop error", e);
              }
              setTimeout(() => startRecordingFlow(), 350);
            }
          });
        }

        // ---------- UI bindings ----------
        micBtn.addEventListener("click", () => {
          if (!recognition && !shouldKeepRecording) {
            // start
            startRecordingFlow();
          } else if (shouldKeepRecording) {
            // stop
            stopRecordingFlow();
          } else {
            // start again
            startRecordingFlow();
          }
        });

        sendBtn.addEventListener("click", () => {
          const text = textInput.value.trim();
          if (!text) return;
          addMessage(text, "user", false);
          textInput.value = "";
        });

        textInput.addEventListener("keypress", (e) => {
          if (e.key === "Enter") {
            sendBtn.click();
          }
        });

        // remove interim on focus
        textInput.addEventListener("focus", () => {
          if (interimElement) {
            interimElement.remove();
            interimElement = null;
          }
        });

        // expose debug helpers (optional)
        window.__voiceChat = {
          startRecordingFlow,
          stopRecordingFlow,
          pushToHistory,
          loadHistory,
        };

        // ---------- init ----------
        renderHistory();
        setStatus("idle");
        dbg("voice chat loaded — click mic to start");
      })();
    </script>
  </body>
</html>
